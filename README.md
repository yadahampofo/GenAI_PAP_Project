# Mutilingual Persuasive Adversarial Prompting Documentation
# PROJECT OVERVIEW

This project buildings upon the paper **How Johnny Can Persuade LLMs to Jailbreak Them:
Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs** by Incorporating a multilingual evaluation to see the sucessrates of Persuasive Adversarial Prompting across different languages. 

# MOTIVATION

Due to the widespread use of LLM across the globe ensuring that LLMs are safety aligned is of utmost priority. Scientists continue to explore vulnerabilities in an attempt to ensure that these pitfalls can immeediately be rectified. Previous publications have shown that Persuasive Adversarial Prompting has been sucessful in jailbreaking using a myriad of persuasion tecniques. However, an evalution of adversarial sucess rates these persuaive techniques across languages is yet to be investigated.

This project aims to bridge the gap in knowlege and explore adversarial sucess rates across three languages (French, English and Korean.)

# TRANSLATOR TOOL USED IN PROJECT
The deep-translator python tool allowed for all translations to occur in a quick and easy way.
- Documentation about the deep-translator can be read below:
- https://deep-translator.readthedocs.io/en/latest/usage.html

# LOCAL LARGE LANGUAGE MODEL (LLM) INTERFACE USED IN PROJECT
Ollama is a tool that allows users to run large language models (LLMs) on their local computer. 
- Documentation about Ollama LLM interface can be read below:
- https://python.langchain.com/docs/integrations/llms/ollama/#usage 

#  Notes & Limitations
<br /> Created using code from https://github.com/CHATS-lab/persuasive_jailbreaker
<br /> Tech stack used: Python, JupyterNotebook <br />
